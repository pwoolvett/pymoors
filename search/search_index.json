{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udcda Welcome to the pymoors Documentation","text":"<p>This is the official documentation for the pymoors project.</p>"},{"location":"#what-is-pymoors","title":"What is pymoors?","text":"<p>pymoors is a library of genetic algorithms for solving multi-objective optimization problems. Its main goal is to provide a faster tool than current Python libraries, as it is built purely in Rust.</p> <p>Genetic Algorithms are exposed to Python via pyo3.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Implemented in Rust for superior performance.</li> <li>Accessible in Python through pyo3.</li> <li>Specialized in solving multi-objective optimization problems using genetic algorithms.</li> </ul>"},{"location":"#introduction-to-multi-objective-optimization","title":"Introduction to Multi-Objective Optimization","text":"<p>Multi-objective optimization refers to a set of techniques and methods designed to solve problems where multiple objectives must be satisfied simultaneously. These objectives are often conflicting, meaning that improving one may deteriorate another. For instance, one might seek to minimize production costs while maximizing product quality at the same time.</p>"},{"location":"#general-formulation","title":"General Formulation","text":"<p>A multi-objective optimization problem can be formulated in a generic mathematical form. If we have \\(k\\) objective functions to optimize, it can be expressed as:</p> \\[ \\begin{aligned} &amp;\\min_x \\quad (f_1(x), f_2(x), \\dots, f_k(x)) \\\\ &amp;\\text{subject to:} \\\\ &amp;g_i(x) \\leq 0, \\quad i = 1, \\dots, m \\\\ &amp;h_j(x) = 0, \\quad j = 1, \\dots, p \\end{aligned} \\] <p>Where: - \\( x \\) represents the set of decision variables. - \\( f_i(x) \\) are the objective functions. - \\( g_i(x) \\leq 0 \\) and \\( h_j(x) = 0 \\) represent the constraints of the problem (e.g., resource limits, quality requirements, etc.).</p> <p>Unlike single-objective optimization, here we seek to optimize all objectives simultaneously. However, in practice, there is no single \u201cbest\u201d solution for all objectives. Instead, we look for a set of solutions known as the Pareto front or Pareto set.</p>"},{"location":"#the-role-of-genetic-algorithms-in-multi-objective-optimization","title":"The Role of Genetic Algorithms in Multi-Objective Optimization","text":"<p>Genetic algorithms (GAs) are search methods inspired by biological evolution techniques, such as selection, crossover, and mutation. Some of their key characteristics include:</p> <ul> <li>Heuristic: They do not guarantee finding the global optimum but can often approximate it efficiently for many types of problems.</li> <li>Parallel Search: They work with a population of potential solutions, which allows them to explore multiple regions of the search space simultaneously.</li> <li>Lightweight: They usually require less information about the problem (e.g., no need for gradients or derivatives), making them suitable for complex or difficult-to-model objective functions.</li> </ul>"},{"location":"#advantages-for-multi-objective-optimization","title":"Advantages for Multi-Objective Optimization","text":"<ol> <li>Natural Handling of Multiple Objectives: By operating on a population of solutions, GAs can maintain an approximation to the Pareto front during execution.</li> <li>Flexibility: They can be easily adapted to different kinds of problems (discrete, continuous, constrained, etc.).</li> <li>Robustness: They tend to perform well in the presence of noise or uncertainty in the problem, offering acceptable performance under less-than-ideal conditions.</li> </ol>"},{"location":"#beauty-and-misbehavior-optimization-problem","title":"Beauty and Misbehavior Optimization Problem","text":"<p>In this unique optimization problem, there is only one individual who optimizes both beauty and misbehavior at the same time: my little dog Arya!</p> <p>Arya not only captivates with her beauty, but she also misbehaves in the most adorable way possible. This problem serves as a reminder that sometimes the optimal solution is as heartwarming as it is delightfully mischievous.</p>"},{"location":"development/","title":"Installing pymoors from source","text":""},{"location":"development/#setting-up","title":"Setting Up","text":"<p>Before you proceed, make sure you have Rust installed. We recommend using rustup for an easy setup:</p> <pre><code># For Linux/Mac:\ncurl --proto '=https' --tlsv1.2 https://sh.rustup.rs -sSf | sh\n\n# For Windows, download and run the installer from the official website:\n# https://rustup.rs/\n</code></pre> <p>Also <code>pymoors</code> uses uv . Make sure it's available on your PATH so the <code>make</code> commands can run properly.</p> <p>Then</p> <ul> <li> <p>Clone the repository <pre><code>git clone https://github.com/andresliszt/pymoors\ncd pymoors\n</code></pre></p> </li> <li> <p>Create a virtual environment <pre><code># Create the virtual environment\nuv venv\n\n# Activate it (Linux/Mac)\nsource .venv/bin/activate\n\n# On Windows, you would typically do:\n# .venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install and compile in dev mode <pre><code>make build-dev\n</code></pre></p> </li> </ul> <p>This command will install all the necessary dependencies for development and compile using maturin.</p> <ul> <li>Compile in prod mode <pre><code>make build-prod\n</code></pre> This command will run <code>maturin develop --release</code>, i.e compiling in a optimized way. This is needed when you want to check the performance of the different algorithms.</li> </ul>"},{"location":"development/#code-formatting-and-linting","title":"Code Formatting and Linting","text":"<p>To ensure code quality and consistency, <code>pymoors</code> uses various tools for formatting and linting. You can run these tools using the provided <code>make</code> commands.</p> <ul> <li> <p>Format the code <pre><code>make format\n</code></pre> This command will format the codebase using tools like <code>black</code> for Python and <code>rustfmt</code> for Rust.</p> </li> <li> <p>Lint the code <pre><code>make lint\n</code></pre> This command will run linters like <code>flake8</code> for Python and <code>clippy</code> for Rust to check for potential issues and enforce coding standards.</p> </li> </ul> <p>Additionally, <code>pymoors</code> uses <code>pre-commit</code> to automatically run these checks before each commit. To set up <code>pre-commit</code>, run the following command:</p> <pre><code>pre-commit install\n</code></pre> <p>This will ensure that your code is formatted and linted before every commit, helping maintain code quality throughout the development process.</p>"},{"location":"getting_started/","title":"Installation","text":"<p>You can install pymoors directly from PyPi doing</p> <pre><code>pip install pymoors\n</code></pre>"},{"location":"getting_started/#example-the-multi-objective-knapsack-problem","title":"Example: The Multi-Objective Knapsack Problem","text":"<p>The multi-objective knapsack problem is a classic example in optimization where we aim to select items, each with its own benefits and costs, subject to certain constraints (e.g., weight capacity). In the multi-objective version, we want to optimize more than one objective function simultaneously\u2014often, maximizing multiple benefits or qualities at once.</p>"},{"location":"getting_started/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>Suppose we have \\(n\\) items. Each item \\(i\\) has: - A profit \\(p_i\\). - A quality \\(q_i\\). - A weight \\(w_i\\).</p> <p>Let \\(x_i\\) be a binary decision variable where \\(x_i = 1\\) if item \\(i\\) is selected and \\(x_i = 0\\) otherwise. We define a knapsack capacity \\(C\\). A common multi-objective formulation for this problem is:</p> \\[ \\begin{aligned} &amp;\\text{Maximize} &amp;&amp; f_1(x) = \\sum_{i=1}^{n} p_i x_i \\\\ &amp;\\text{Maximize} &amp;&amp; f_2(x) = \\sum_{i=1}^{n} q_i x_i \\\\ &amp;\\text{subject to} &amp;&amp; \\sum_{i=1}^{n} w_i x_i \\leq C,\\\\ &amp; &amp;&amp; x_i \\in \\{0, 1\\}, \\quad i = 1,\\dots,n. \\end{aligned} \\]"},{"location":"getting_started/#solving-it-with-pymoors","title":"Solving it with <code>pymoors</code>","text":"<pre><code>import numpy as np\n\nfrom pymoors import (\n    Nsga2,\n    RandomSamplingBinary,\n    BitFlipMutation,\n    SinglePointBinaryCrossover,\n    ExactDuplicatesCleaner,\n)\nfrom pymoors.typing import TwoDArray\n\n\nPROFITS = np.array([2, 3, 6, 1, 4])\nQUALITIES = np.array([5, 2, 1, 6, 4])\nWEIGHTS = np.array([2, 3, 6, 2, 3])\nCAPACITY = 7\n\n\ndef knapsack_fitness(genes: TwoDArray) -&gt; TwoDArray:\n    # Calculate total profit\n    profit_sum = np.sum(PROFITS * genes, axis=1, keepdims=True)\n    # Calculate total quality\n    quality_sum = np.sum(QUALITIES * genes, axis=1, keepdims=True)\n\n    # We want to maximize profit and quality,\n    # so in pymoors we minimize the negative values\n    f1 = -profit_sum\n    f2 = -quality_sum\n    return np.column_stack([f1, f2])\n\n\ndef knapsack_constraint(genes: TwoDArray) -&gt; TwoDArray:\n    # Calculate total weight\n    weight_sum = np.sum(WEIGHTS * genes, axis=1, keepdims=True)\n    # Inequality constraint: weight_sum &lt;= capacity\n    return weight_sum - CAPACITY\n\n\nalgorithm = Nsga2(\n    sampler=RandomSamplingBinary(),\n    crossover=SinglePointBinaryCrossover(),\n    mutation=BitFlipMutation(gene_mutation_rate=0.5),\n    fitness_fn=knapsack_fitness,\n    constraints_fn=knapsack_constraint,\n    duplicates_cleaner=ExactDuplicatesCleaner(),\n    n_vars=5,\n    pop_size=32,\n    n_offsprings=32,\n    num_iterations=100,\n    mutation_rate=0.1,\n    crossover_rate=0.9,\n    keep_infeasible=False,\n)\n\nalgorithm.run()\n</code></pre> <p>In this small example, the algorithm finds a single solution on the Pareto front: selecting the items (A, D, E), with a profit of 7 and a quality of 15. This means there is no other combination that can match or exceed both objectives without exceeding the knapsack capacity (7).</p> <p>Once the algorithm finishes, it stores a <code>population</code> attribute that contains all the individuals evaluated during the search.</p> <pre><code>pop = algorithm.population\n# Get genes\n&gt;&gt;&gt; pop.genes\narray([[1., 0., 0., 1., 1.],\n       [0., 1., 0., 0., 1.],\n       [1., 1., 0., 1., 0.],\n       [0., 0., 0., 1., 1.],\n       [1., 0., 0., 0., 1.],\n       [1., 0., 0., 1., 0.],\n       [1., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 1., 0., 1., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]])\n# Get fitness\n&gt;&gt;&gt; pop.fitness\narray([[ -7., -15.],\n       [ -7.,  -6.],\n       [ -6., -13.],\n       [ -5., -10.],\n       [ -6.,  -9.],\n       [ -3., -11.],\n       [ -5.,  -7.],\n       [ -6.,  -1.],\n       [ -4.,  -8.],\n       [ -2.,  -5.],\n       [ -1.,  -6.],\n       [ -4.,  -4.],\n       [ -3.,  -2.],\n       [ -0.,  -0.]])\n# Get constraints evaluation\n&gt;&gt;&gt; pop.constraints\narray([[ 0.],\n       [-1.],\n       [ 0.],\n       [-2.],\n       [-2.],\n       [-3.],\n       [-2.],\n       [-1.],\n       [-2.],\n       [-5.],\n       [-5.],\n       [-4.],\n       [-4.],\n       [-7.]])\n# Get rank\n&gt;&gt;&gt; pop.rank\narray([0, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 6], dtype=uint64)\n</code></pre> <p>Note that in this example there is just one individual with rank 0, i.e Pareto optimal. Algorithms in <code>pymoors</code> store all individuals with rank 0 in a special attribute <code>best</code>, which is list of  <code>pymoors.schemas.Individual</code> objects</p> <pre><code># Get best individuals\nbest = pop.best\n&gt;&gt;&gt; best\n[&lt;pymoors.schemas.Individual object at 0x11b8ec110&gt;]\n# In this small exmaple as mentioned, best is just one single individual (A, D, E)\n&gt;&gt;&gt; best[0].genes\narray([1., 0., 0., 1., 1.])\n&gt;&gt;&gt; best[0].fitness\narray([ -7., -15.])\n&gt;&gt;&gt; best[0].constraints\narray([0.])\n</code></pre> <p>Population Size and Duplicates</p> <p>Note that although the specified <code>pop_size</code> was 32, the final population ended up being 13 individuals, of which 1 had <code>rank = 0</code>. This is because we used the <code>keep_infeasible=False</code> argument, removing any individual that did not satisfy the constraints (in this case, the weight constraint). We also used a duplicate remover called <code>ExactDuplicatesCleaner</code> that eliminates all exact duplicates\u2014 meaning whenever <code>genes1 == genes2</code> in every component.</p> <p>Variable Types in pymoors</p> <p>In pymoors, there is no strict enforcement of whether variables are integer, binary, or real. The core Rust implementation works with <code>f64</code> ndarrays. To preserve a specific variable type\u2014binary, integer, or real\u2014you must ensure that the genetic operators themselves maintain it.  </p> <p>It is the user's responsibility to choose the appropriate genetic operators for the variable type in question. In the knapsack example, we use binary-style genetic operators, which is why the solutions are arrays of 0s and 1s.</p>"},{"location":"getting_started/#example-a-real-valued-multi-objective-optimization-problem","title":"Example: A Real-Valued Multi-Objective Optimization Problem","text":"<p>Below is a simple two-variable multi-objective problem to illustrate real-valued optimization with <code>pymoors</code>. We have two continuous decision variables, \\( x_1 \\) and \\( x_2 \\), both within a given range. We define two objective functions to be minimized simultaneously, and we solve this using the popular NSGA2 algorithm.</p>"},{"location":"getting_started/#mathematical-formulation_1","title":"Mathematical Formulation","text":"<p>Let \\(\\mathbf{x} = (x_1, x_2)\\) be our decision variables, each constrained to the interval \\([-2, 2]\\). We define the following objectives:</p> \\[ \\begin{aligned} \\min_{\\mathbf{x}} \\quad &amp;f_1(x_1, x_2) = x_1^2 + x_2^2 \\\\ \\min_{\\mathbf{x}} \\quad &amp;f_2(x_1, x_2) = (x_1 - 1)^2 + x_2^2 \\\\ \\text{subject to} \\quad &amp; -2 \\le x_1 \\le 2, \\\\ &amp; -2 \\le x_2 \\le 2. \\end{aligned} \\] <p>Interpretation</p> <ol> <li>\\(f_1\\) measures the distance of \\(\\mathbf{x}\\) from the origin \\((0,0)\\) in the 2D plane.</li> <li>\\(f_2\\) measures the distance of \\(\\mathbf{x}\\) from the point \\((1,0)\\).</li> </ol> <p>Thus, \\(\\mathbf{x}\\) must compromise between being close to \\((0,0)\\) and being close to \\((1,0)\\). There is no single point in \\([-2,2]^2\\) that simultaneously minimizes both distances perfectly (other than at the boundary of these trade-offs), so we end up with a Pareto front rather than a single best solution.</p>"},{"location":"getting_started/#solving-it-with-pymoors_1","title":"Solving it with <code>pymoors</code>","text":"<p>Below is a complete example in Python demonstrating how to set up this problem using and solve it with the NSGA2 genetic algorithm.</p> <pre><code>import numpy as np\n\nfrom pymoors import (\n    Nsga2,\n    RandomSamplingFloat,\n    GaussianMutation,\n    ExponentialCrossover,\n    CloseDuplicatesCleaner\n)\nfrom pymoors.typing import TwoDArray\n\n\ndef fitness(genes: TwoDArray) -&gt; TwoDArray:\n    x1 = genes[:, 0]\n    x2 = genes[:, 1]\n    # Objective 1: Distance to (0,0)\n    f1 = x1**2 + x2**2\n    # Objective 2: Distance to (1,0)\n    f2 = (x1 - 1)**2 + x2**2\n    # Combine the two objectives into a single array\n    return np.column_stack([f1, f2])\n\n\n\nalgorithm = Nsga2(\n    sampler=RandomSamplingFloat(min = -2, max = 2),\n    crossover=ExponentialCrossover(exponential_crossover_rate = 0.75),\n    mutation=GaussianMutation(gene_mutation_rate=0.1, sigma=0.01),\n    fitness_fn=fitness,\n    duplicates_cleaner=CloseDuplicatesCleaner(epsilon=1e-5),\n    n_vars=2,\n    pop_size=200,\n    n_offsprings=200,\n    num_iterations=200,\n    mutation_rate=0.1,\n    crossover_rate=0.9,\n    keep_infeasible=False,\n    lower_bound=-2,\n    upper_bound=2\n)\n\nalgorithm.run()\n</code></pre> <p>This simple problem has a known Pareto Optimal</p> <ul> <li> <p>Decision-Space Pareto Front: \\( \\{ (x_1, 0) \\mid x_1 \\in [0,1] \\} \\)</p> </li> <li> <p>Objective-Space Pareto Front: \\( \\{ (t^2, (t-1)^2) \\mid t \\in [0,1] \\} \\)</p> </li> </ul> <p>Each point on that curve represents a different trade-off between minimizing the distance to \\((0,0)\\) and to \\((1,0)\\).</p> <p></p>"},{"location":"user_guide/algorithms/","title":"Algorithms","text":"<p>In pymoors, the algorithms are implemented as classes that are exposed on the Python side with a set of useful attributes. These attributes include the final population and the optimal or best set of individuals found during the optimization process.</p> <p>For example, after running an algorithm like NSGA2, you can access: - Final Population: The complete set of individuals from the last generation. - Optimum Set: Typically, the best individuals (e.g., those with rank 0) that form the current approximation of the Pareto front.</p> <p>This design abstracts away the complexities of the underlying Rust implementation and provides an intuitive, Pythonic interface for setting up, executing, and analyzing multi-objective optimization problems.</p>"},{"location":"user_guide/algorithms/#mathematical-formulation-of-a-multi-objective-optimization-problem-with-constraints","title":"Mathematical Formulation of a Multi-Objective Optimization Problem with Constraints","text":"<p>Consider the following optimization problem:</p> \\[ \\begin{aligned} \\min_{x_1, x_2} \\quad &amp; f_1(x_1,x_2) = x_1^2 + x_2^2 \\\\ \\min_{x_1, x_2} \\quad &amp; f_2(x_1,x_2) = (x_1-1)^2 + x_2^2 \\\\ \\text{subject to} \\quad &amp; x_1 + x_2 \\leq 1, \\\\ &amp; x_1 \\geq 0,\\quad x_2 \\geq 0. \\end{aligned} \\]"},{"location":"user_guide/algorithms/#theoretical-solution","title":"Theoretical Solution","text":"<p>The feasible region is defined by the constraints: - \\( x_1 + x_2 \\leq 1 \\), which creates a triangular region in the first quadrant with vertices at \\((0,0)\\), \\((1,0)\\), and \\((0,1)\\). - \\( x_1 \\geq 0 \\) and \\( x_2 \\geq 0 \\).</p> <p>For the objectives: - The first objective, \\( f_1(x_1,x_2) = x_1^2 + x_2^2 \\), is minimized at \\((0,0)\\). - The second objective, \\( f_2(x_1,x_2) = (x_1-1)^2 + x_2^2 \\), is minimized at \\((1,0)\\).</p> <p>Since these two minimizers lie at different vertices of the feasible region, there is an inherent trade-off between the objectives. The Pareto front is formed by the set of non-dominated solutions along the boundary of the feasible region.</p> <p>A common approach to derive the Pareto front is to consider solutions along the line \\( x_1 + x_2 = 1 \\) (with \\(x_1, x_2 \\in [0,1]\\)). By parameterizing the boundary as \\( x_2 = 1 - x_1 \\), we can express the objectives as functions of \\( x_1 \\):</p> \\[ \\begin{aligned} f_1(x_1, 1-x_1) &amp;= x_1^2 + (1-x_1)^2, \\\\ f_2(x_1, 1-x_1) &amp;= (x_1-1)^2 + (1-x_1)^2. \\end{aligned} \\] <p>Thus, the Pareto front is given by:</p> \\[ \\left\\{ \\left(f_1(x_1, 1-x_1),\\, f_2(x_1, 1-x_1)\\right) \\mid x_1 \\in [0,1] \\right\\}. \\] <p>This continuous set of solutions along the boundary represents the trade-off between minimizing \\( f_1 \\) and \\( f_2 \\) within the given constraints.</p> <p>Below is how you can formulate and solve this problem in pymoors:</p> <pre><code>import numpy as np\nfrom pymoors import (\n    Nsga2,\n    RandomSamplingFloat,\n    GaussianMutation,\n    ExponentialCrossover,\n    CloseDuplicatesCleaner\n)\nfrom pymoors.typing import TwoDArray\n\n# Define the fitness function\ndef fitness(genes: TwoDArray) -&gt; TwoDArray:\n    x1 = genes[:, 0]\n    x2 = genes[:, 1]\n    # Objective 1: f1(x1,x2) = x1^2 + x2^2\n    f1 = x1**2 + x2**2\n    # Objective 2: f2(x1,x2) = (x1-1)^2 + x2**2\n    f2 = (x1 - 1)**2 + x2**2\n    return np.column_stack([f1, f2])\n\n# Define the constraints function\ndef constraints(genes: TwoDArray) -&gt; TwoDArray:\n    x1 = genes[:, 0]\n    x2 = genes[:, 1]\n    # Constraint 1: x1 + x2 &lt;= 1\n    g1 = x1 + x2 - 1\n    # Convert to 2D array\n    return g1.reshape(-1, 1)\n\n# Set up the NSGA2 algorithm with the above definitions\nalgorithm = Nsga2(\n    sampler=RandomSamplingFloat(min=0, max=1),\n    crossover=ExponentialCrossover(exponential_crossover_rate=0.75),\n    mutation=GaussianMutation(gene_mutation_rate=0.1, sigma=0.01),\n    fitness_fn=fitness,\n    constraints_fn=constraints,  # Pass the constraints function\n    duplicates_cleaner=CloseDuplicatesCleaner(epsilon=1e-5),\n    n_vars=2,\n    pop_size=200,\n    n_offsprings=200,\n    num_iterations=200,\n    mutation_rate=0.1,\n    crossover_rate=0.9,\n    keep_infeasible=False,\n    lower_bound=0\n)\n\n# Run the algorithm\nalgorithm.run()\n</code></pre>"},{"location":"user_guide/algorithms/#early-stopping-and-termination-conditions","title":"Early Stopping and Termination Conditions","text":"<p>We're currently developing early stopping criteria \u2013 see this open issue \u2013 to terminate the algorithm when, under certain logic, it is considered finished and will no longer find better individuals to optimize.</p> <p>There are also other conditions that will cause the algorithm to terminate with an error. For example, if <code>keep_infeasible</code> is set to <code>False</code> and at some point the population becomes completely infeasible (i.e., there is no individual that satisfies the defined constraints), then pymoo will throw a <code>NoFeasibleIndividualsError</code>.</p>"},{"location":"user_guide/duplicates/","title":"Duplicates Cleaner","text":"<p>In genetic algorithms, one way to maintain diversity is to eliminate duplicates generation after generation. This operation can be computationally expensive but is often essential to ensure that the algorithm continues to explore new individuals that can improve the objectives. Currently, pymoors includes two pre-defined duplicate cleaners.</p>"},{"location":"user_guide/duplicates/#exact-duplicates-cleaner","title":"Exact Duplicates Cleaner","text":"<p>Based on exact elimination, meaning that two individuals (genes1 and genes2) are considered duplicates if and only if each element in genes1 is equal to the corresponding element in genes2. Internally, this cleaner uses Rust\u2019s HashSet, which operates extremely fast for duplicate elimination.</p> <p>Warning</p> <p>Benchmarks comparing pymoors with other Python libraries will be published soon. These benchmarks will highlight the importance and performance impact of duplicate elimination in genetic algorithms.</p> <pre><code>from pymoors import ExactDuplicatesCleaner\n\ncleaner = ExactDuplicatesCleaner()\n</code></pre>"},{"location":"user_guide/duplicates/#closeduplicatescleaner","title":"CloseDuplicatesCleaner","text":"<p>CloseDuplicatesCleaner is designed for real-valued problems where two individuals are very similar, but not exactly identical. In such cases, it is beneficial to consider them as duplicates based on a proximity metric\u2014typically the Euclidean distance. pymoors implements a cleaner of this style that uses the square of the Euclidean distance between two individuals and considers them duplicates if this value is below a configurable tolerance (epsilon).</p> <pre><code>from pymoors import CloseDuplicatesCleaner\n\ncleaner = CloseDuplicatesCleaner(epsilon=1e-5)\n</code></pre> <p>Caution</p> <p>This duplicate elimination algorithm can be computationally expensive when the population size and the number of offsprings are large, because it requires calculating the distance matrix among offsprings first, and then between offsprings and the current population to ensure duplicate elimination using this criterion. The algorithm has a complexity of O(n*m) where n is the population size and m is the number of offsprings.</p>"},{"location":"user_guide/fitness_and_constraints/","title":"Fitness Function in pymoors","text":"<p>In pymoors, the way to define objective functions for optimization is through a NumPy-based function that operates on an entire population. This means that the provided function, <code>f(genes)</code>, expects <code>genes</code> to be a 2D NumPy array with dimensions <code>(pop_size, n_vars)</code>. It must then return a 2D NumPy array of shape <code>(pop_size, n_objectives)</code>, where each row corresponds to the evaluation of a single individual.</p> <p>This population-level evaluation is very important\u2014it allows the algorithm to efficiently process and compare many individuals at once. When writing your fitness function, make sure it is vectorized and returns one row per individual, where each row contains the evaluated objective values.</p> <p>Below is an example fitness function:</p> <pre><code>import numpy as np\nfrom pymoors.typing import TwoDArray\n\ndef fitness(genes: TwoDArray) -&gt; TwoDArray:\n    # Extract decision variables for each individual\n    x1 = genes[:, 0]\n    x2 = genes[:, 1]\n\n    # Objective 1: Distance to (0,0)\n    f1 = x1**2 + x2**2\n\n    # Objective 2: Distance to (1,0)\n    f2 = (x1 - 1)**2 + x2**2\n\n    # Combine the two objectives into a single array,\n    # where each row is the evaluation for one individual\n    return np.column_stack([f1, f2])\n</code></pre> <p>Note that we have an special type alias <code>pymoors.typing.TwoDArray</code> which is no other thing than <code>Annotated[npt.NDArray[DType], \"ndim=2\"]</code>. This alias emphasizes that the array must be two-dimensional.</p>"},{"location":"user_guide/fitness_and_constraints/#minimization-and-maximization","title":"Minimization and Maximization","text":"<p>In pymoors\u2014as with many optimization frameworks\u2014the core approach is based on minimization. This means that the optimization algorithm is designed to search for solutions that yield the lowest possible objective function values.</p> <p>When you encounter a problem where you want to maximize an objective (for example, maximize profit or efficiency), you can simply convert it into a minimization problem. This is achieved by taking the negative of the objective function. In effect, maximizing an objective is equivalent to minimizing its negative value.</p>"},{"location":"user_guide/fitness_and_constraints/#key-points","title":"Key Points","text":"<ul> <li> <p>Default Minimization:   pymoors inherently minimizes objective functions. Lower values are considered better, and the optimization process works to reduce these values.</p> </li> <li> <p>Converting Maximization to Minimization:   To handle maximization, you multiply the objective function by -1. By doing so, you transform a maximization problem into a minimization one. This unified approach simplifies the optimization framework.</p> </li> <li> <p>Practical Considerations:   When defining your fitness functions, ensure that the returned evaluations conform to this minimization paradigm. For objectives that are originally maximization problems, adjust them by negating their outputs before they are returned by the fitness function.</p> </li> </ul> <p>This method ensures a consistent and streamlined optimization process within <code>pymoors</code>.</p>"},{"location":"user_guide/fitness_and_constraints/#constraints","title":"Constraints","text":"<p>Constraints in an optimization problem are optional. They are defined using a similar approach to the fitness function. In pymoors, you define a constraint function <code>g(genes)</code> where <code>genes</code> is a 2D array of shape <code>(pop_size, n_vars)</code>, and the function must return a 2D array of shape <code>(pop_size, n_constraints)</code>. Each row of the output corresponds to the constraint evaluations for an individual in the population.</p> <p>Feasibility of an Individual</p> <p>In pymoors, an individual in the population is considered feasible if and only if all constraints are less than or equal to 0.  </p> <p>In the following subsections, we will explain how to consider other types of inequalities.</p> <p>Below is an example constraint function. In this example, we enforce a simple constraint: the sum of the decision variables for each individual must be less than or equal to a specified threshold value.</p> <pre><code>import numpy as np\nfrom pymoors.typing import TwoDArray\n\ndef constraints(genes: TwoDArray) -&gt; TwoDArray:\n    # Define a threshold for the sum of genes\n    threshold = 10\n\n    # Compute the sum for each individual (row)\n    row_sums = np.sum(genes, axis=1)\n\n    # Constraint: row_sums should be less than or equal to threshold.\n    # We compute the violation as (row_sums - threshold). A value &lt;= 0 means the constraint is satisfied.\n    violations = row_sums - threshold\n\n    # Return as a 2D array with one constraint per individual\n    return violations.reshape(-1, 1)\n</code></pre>"},{"location":"user_guide/fitness_and_constraints/#key-points_1","title":"Key points","text":"<ul> <li> <p>Input: The function receives <code>genes</code>, a 2D array with shape <code>(pop_size, n_vars)</code>, where each row represents an individual in the population.</p> </li> <li> <p>Constraint Calculation: For each individual, the function calculates the sum of its decision variables. The constraint is defined such that the sum must be less than or equal to a specified threshold (10 in this example).</p> </li> <li>If the sum is less than or equal to the threshold, the constraint value (i.e., the violation) is \u2264 0, meaning the constraint is satisfied.</li> <li> <p>If the sum exceeds the threshold, the resulting positive value indicates the magnitude of the violation.</p> </li> <li> <p>Output: The function returns a 2D array of shape <code>(pop_size, 1)</code>, where each element represents the constraint evaluation for the corresponding individual.</p> </li> <li> <p>Note: The use of <code>reshape(-1, 1)</code> is crucial for ensuring that the output always has the correct dimensions, even when there is only one constraint. This guarantees consistency in the dimensionality of the constraint evaluations.</p> </li> </ul>"},{"location":"user_guide/fitness_and_constraints/#handling-constraints-greater-than-0-and-equality-constraints","title":"Handling Constraints: Greater than 0 and Equality Constraints","text":"<p>In some optimization problems, constraints might be defined as either inequalities of the form <code>g(genes) &gt; 0</code> or as equality constraints <code>g(genes) = 0</code>. In pymoors, since feasibility is determined by having all constraint values \\(\u2264 0\\), we handle these cases as follows:</p> <ul> <li> <p>Inequalities (<code>g(genes) &gt; 0</code>):   This case is trivial to convert. Simply multiply the output of your constraint function by -1 to transform it into the standard form (<code>g(genes) &lt;= 0</code>).</p> </li> <li> <p>Equality Constraints (<code>g(genes) = 0</code>) with Tolerance:   Equality constraints are typically managed by allowing a small tolerance, <code>epsilon</code>, around 0. A common approach is to construct a penalty function by computing the squared deviation <code>(g(genes) - epsilon)\u00b2</code>. This squared term penalizes any deviation from the desired equality, while the tolerance <code>epsilon</code> provides some leeway for numerical imprecision.</p> </li> </ul>"},{"location":"user_guide/fitness_and_constraints/#example-equality-constraint-with-epsilon-tolerance","title":"Example: Equality Constraint with Epsilon Tolerance","text":"<p>Below is an example constraint function. In this example, we enforce that the sum of decision variables for each individual should equal a specified threshold (10) within a tolerance of <code>epsilon</code>. The function computes the squared error from the target (adjusted by <code>epsilon</code>) and ensures the output is a 2D array using <code>reshape(-1, 1)</code>.</p> <pre><code>import numpy as np\nfrom pymoors.typing import TwoDArray\n\nTOL = 1e-3\n\ndef constraints_equality(genes: TwoDArray) -&gt; TwoDArray:\n    threshold = 10\n    row_sums = np.sum(genes, axis=1)\n    # Compute the squared error from the equality constraint with tolerance epsilon.\n    error = (row_sums - threshold - TOL)**2\n    # The reshape ensures the output is a 2D array of shape (pop_size, 1)\n    return error.reshape(-1, 1)\n</code></pre>"},{"location":"user_guide/operators/","title":"Genetic Operators","text":"<p>In pymoors, the genetic operators expected by the various algorithms are mutation, crossover, and a sampler. The selection and tournament operators are usually built into the algorithms; for example, NSGA2 employs selection and tournament procedures based on Rank and Crowding.</p> <p>Currently, pymoors comes with a battery of pre-defined genetic operators implemented in Rust. The goal is to continuously add more classic genetic operators to reduce the amount of Python code that needs to be executed.</p> <p>Warning</p> <p>At the moment, pymoors does not provide a way to define custom genetic operators using NumPy functions. This feature is planned to be available as soon as possible.</p> <p>Each genetic operator in pymoors is exposed to Python as a class. For example, consider the following:</p> <pre><code>from pymoors import RandomFloatSampling, GaussianMutation, ExponentialCrossover,\n\n# Create a sampler that generates individuals randomly between 0 and 10.\nsampler = RandomFloatSampling(min=0, max=10)\n# Create a gauss mutator instance\nmutation=GaussianMutation(gene_mutation_rate=0.1, sigma=0.01)\n# Create an exponential crossover instance\ncrossover=ExponentialCrossover(exponential_crossover_rate = 0.75)\n</code></pre> <p>Warning</p> <ol> <li>Currently, these instances serve only as a means to inform Rust which operator to use, but they do not expose any public methods to Python; everything functions internally within the Rust core. We are currently evaluating whether it is necessary to expose these methods to Python so that users can interact with them.</li> <li>This section will be updated with more information on all the genetic operators in the near future.</li> </ol>"},{"location":"user_guide/operators/#check-available-python-rust-operators","title":"Check available python rust operators","text":"<p><code>pymoors</code> provides a convenient method called <code>available_operators</code> that allows you to check which operators are implemented and exposed from Rust to Python. This includes operators for sampling, crossover, mutation and duplicates cleaner selection, and survival.</p> <pre><code>from pymoors import available operators\n\n&gt;&gt;&gt; available_operators(operator_type = \"mutation\", include_docs = True)\n{'BitFlipMutation': 'Mutation operator that flips bits in a binary individual with a specified mutation rate.', 'SwapMutation':  ...} # The dictionary was shortened for simplicity.\n</code></pre> <p><code>operator_type</code> must be <code>'sampling'</code>, <code>'crossover'</code>, <code>'mutation'</code> or <code>'duplicates'</code>. Also the parameter <code>include_docs</code> includes the first line of the operator docstring, if it's set as <code>False</code> will return a list with class names only.</p>"}]}